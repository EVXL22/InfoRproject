{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36547a40-941c-44b2-beef-e785b382f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\inforetrieval\\mynewenv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: nltk in c:\\inforetrieval\\mynewenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 3.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/12.6 MB 3.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 3.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.9/12.6 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.6 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.0/12.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.3/12.6 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.6/12.6 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.7/12.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.2/12.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 2.7 MB/s eta 0:00:00\n",
      "Using cached scipy-1.15.1-cp313-cp313-win_amd64.whl (43.6 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, scipy, scikit-learn\n",
      "Successfully installed numpy-2.2.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd981f-7ef7-4ba9-9f73-6ed8b9e4f07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da711bf-9d6c-4cea-8ebf-5e758b96bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα άρθρα αποθηκεύτηκαν επιτυχώς στο wikipedia_articles.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def fetch_wikipedia_articles(num_articles=10):\n",
    "    base_url = 'https://en.wikipedia.org/wiki/Special:Random'\n",
    "    articles = []\n",
    "\n",
    "    for _ in range(num_articles):\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Εξαγωγή τίτλου και περιεχομένου άρθρου\n",
    "        title = soup.find('h1').text\n",
    "        content = ' '.join([p.text for p in soup.find_all('p')])\n",
    "        \n",
    "        # Προσθήκη άρθρου στη λίστα\n",
    "        articles.append({'title': title, 'content': content})\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Συλλογή 10 άρθρων\n",
    "articles = fetch_wikipedia_articles(num_articles=10)\n",
    "\n",
    "# Αποθήκευση άρθρων σε αρχείο JSON\n",
    "with open('wikipedia_articles.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(\"Τα άρθρα αποθηκεύτηκαν επιτυχώς στο wikipedia_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da02595d-9ef1-430b-a667-a1af4606e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Althea Gwyn\n",
      "Althea R. Gwyn (May 19, 1956 – January 9, 2022) was an American professional basketball player who was one of the first players in the Women's Professional Basketball League (WBL).[1][2]\n",
      " Gwyn started\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Vietnam water snake\n",
      "\n",
      " Pararhabdophis chapaensis Bourret, 1934\n",
      " The Vietnam water snake or Chapa flat-nosed snake (Hebius chapaensis) is a species of colubrid snake.[1][2] It is found in northern Vietnam[1][2] and Yunnan,\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Utah State Route 12\n",
      "\n",
      " State Route 12 or Scenic Byway 12 (SR-12), also known as \"Highway 12 — A Journey Through Time Scenic Byway\", is a 123-mile-long (198 km) state highway designated an All-American Road located in Garf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Roland Stănescu\n",
      "\n",
      " Roland Dedius Stănescu (19 April 1990 – 2 July 2022) was a Romanian footballer who played as a midfielder. He played mostly in the Liga II for Dacia Unirea Braila, CS Balotești, CS Minerul Motru and\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Bedwellty Greyhound Track\n",
      "\n",
      " Bedwellty Greyhound Track was a greyhound racing track in the hamlet of Bedwellty, near Aberbargoed, South Wales. It was sometimes called Bedwellty Park.[1]\n",
      " The track was situated north of the Bedw\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mamitha Baiju\n",
      "\n",
      " Mamitha Baiju is an Indian actress working primarily in Tamil films in addition to Malayalam films. She made her debut in 2018 through Sarvopari Palakkaran.[1][2] She is well known for her character\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Jane Symons\n",
      "\n",
      "Jane Symons (born 1959) is an Australian media consultant, journalist and author based in London. She has written for a wide range of newspapers and magazines, and she is a vice chair of the Medical \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Expected satiety\n",
      "Expected satiety is the amount of relief from hunger that is expected from a particular food. It is closely associated with expected satiation which refers to the immediate fullness (post meal) that a\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Martin Jeitziner\n",
      "\n",
      " Martin Jeitziner (born 13 January 1963) is a Swiss former professional footballer who played as a midfielder in the 1980s and 1990s.[1]\n",
      " Jeitziner played his youth football with FC Basel and advance\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thomaston Historic District\n",
      "\n",
      " The Thomaston Historic District encompasses much of the historic town center of Thomaston, Maine.  With a settlement history dating to the 17th century, the town is now a showcase of 19th-century ar\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Εμφάνιση των πρώτων 200 χαρακτήρων κάθε άρθρου\n",
    "for article in articles:\n",
    "    print(article['title'])\n",
    "    print(article['content'][:200])\n",
    "    print('\\n' + '-'*80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e18b4d-3bbe-4b71-8010-6f3245f698e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\xilou/nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\share\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\xilou\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Προεπεξεργασία κειμένων\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[1;32m---> 37\u001b[0m     article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mΤα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     24\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Αφαίρεση stop-words\u001b[39;00m\n\u001b[0;32m     28\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\xilou/nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\share\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\xilou\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Κατέβασμα πρόσθετων δεδομένων από το nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Φόρτωση των άρθρων\n",
    "with open('wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Ορισμός λειτουργιών για προεπεξεργασία κειμένου\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Αφαίρεση ειδικών χαρακτήρων\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Αφαίρεση stop-words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Προεπεξεργασία κειμένων\n",
    "for article in articles:\n",
    "    article['processed_tokens'] = preprocess_text(article['content'])\n",
    "\n",
    "print(\"Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f278d2-a297-430c-8104-32568e490343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a0dda6-11da-4904-bf96-1134d06fda06",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\xilou/nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\share\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\xilou\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Προεπεξεργασία κειμένων\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[1;32m---> 31\u001b[0m     article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mΤα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     18\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Αφαίρεση stop-words\u001b[39;00m\n\u001b[0;32m     22\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\InfoRetrieval\\mynewenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\xilou/nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\share\\\\nltk_data'\n    - 'c:\\\\InfoRetrieval\\\\mynewenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\xilou\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Φόρτωση των άρθρων\n",
    "with open('wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Ορισμός λειτουργιών για προεπεξεργασία κειμένου\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Αφαίρεση ειδικών χαρακτήρων\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Αφαίρεση stop-words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Προεπεξεργασία κειμένων\n",
    "for article in articles:\n",
    "    article['processed_tokens'] = preprocess_text(article['content'])\n",
    "\n",
    "print(\"Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf70f8dd-2767-40b8-bc56-c0b426149ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\xilou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33e16268-ecc5-4425-8d4e-ee1def8098cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Φόρτωση των άρθρων\n",
    "with open('wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Ορισμός λειτουργιών για προεπεξεργασία κειμένου\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Αφαίρεση ειδικών χαρακτήρων\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Αφαίρεση stop-words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Προεπεξεργασία κειμένων\n",
    "for article in articles:\n",
    "    article['processed_tokens'] = preprocess_text(article['content'])\n",
    "\n",
    "print(\"Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89cc60a3-14df-49d6-a2da-2cc373fb205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το ανεστραμμένο ευρετήριο δημιουργήθηκε επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "for doc_id, article in enumerate(articles):\n",
    "    for word in article['processed_tokens']:\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(doc_id)\n",
    "\n",
    "print(\"Το ανεστραμμένο ευρετήριο δημιουργήθηκε επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e14562ec-d7fd-4a50-871c-c9030b1f5a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης για το 'example query': []\n"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    query_tokens = preprocess_text(query)\n",
    "    result = set(inverted_index.get(query_tokens[0], []))\n",
    "    \n",
    "    for token in query_tokens[1:]:\n",
    "        result &= set(inverted_index.get(token, []))\n",
    "    \n",
    "    return list(result)\n",
    "\n",
    "# Παράδειγμα αναζήτησης\n",
    "search_results = search(\"example query\")\n",
    "print(\"Αποτελέσματα αναζήτησης για το 'example query':\", search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09bf5df2-9fd1-43e0-899f-098a0f156e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Άρθρο 1: Althea Gwyn\n",
      "Επεξεργασμένες Λέξεις: ['Althea', 'R', 'Gwyn', 'May', '19', '1956', 'January', '9', '2022', 'American', 'professional', 'basketball', 'player', 'one', 'first', 'player', 'Womens', 'Professional', 'Basketball', 'League', 'WBL12', 'Gwyn', 'started', 'basketball', 'career', 'Amityville', 'Memorial', 'High', 'School', 'Long']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Άρθρο 2: Vietnam water snake\n",
      "Επεξεργασμένες Λέξεις: ['Pararhabdophis', 'chapaensis', 'Bourret', '1934', 'The', 'Vietnam', 'water', 'snake', 'Chapa', 'flatnosed', 'snake', 'Hebius', 'chapaensis', 'specie', 'colubrid', 'snake12', 'It', 'found', 'northern', 'Vietnam12', 'Yunnan', 'southern', 'China2', 'This', 'article', 'relating', 'Natricinae', 'stub', 'You', 'help']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Άρθρο 3: Utah State Route 12\n",
      "Επεξεργασμένες Λέξεις: ['State', 'Route', '12', 'Scenic', 'Byway', '12', 'SR12', 'also', 'known', 'Highway', '12', 'A', 'Journey', 'Through', 'Time', 'Scenic', 'Byway', '123milelong', '198', 'km', 'state', 'highway', 'designated', 'AllAmerican', 'Road', 'located', 'Garfield', 'County', 'Wayne', 'County']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Εμφάνιση των πρώτων 3 άρθρων και των επεξεργασμένων λέξεών τους\n",
    "for i, article in enumerate(articles[:3]):\n",
    "    print(f\"Άρθρο {i+1}: {article['title']}\")\n",
    "    print(f\"Επεξεργασμένες Λέξεις: {article['processed_tokens'][:30]}\")  # Δείχνει τις πρώτες 30 λέξεις\n",
    "    print('\\n' + '-'*80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27646ace-a139-498f-bac3-2a0f78b6069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης για το 'example': [9]\n",
      "Αποτελέσματα αναζήτησης για το 'query': []\n",
      "Αποτελέσματα αναζήτησης για το 'search': []\n",
      "Αποτελέσματα αναζήτησης για το 'article': [1, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμαστική αναζήτηση με διαφορετικά ερωτήματα\n",
    "queries = [\"example\", \"query\", \"search\", \"article\"]\n",
    "for query in queries:\n",
    "    results = search(query)\n",
    "    print(f\"Αποτελέσματα αναζήτησης για το '{query}': {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55b952de-45e9-4c32-84e4-7340d277c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης για το 'example AND query': []\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    # Διαχωρισμός των όρων και των λογικών συνδέσμων\n",
    "    terms = query.split()\n",
    "    result_set = set()\n",
    "    \n",
    "    if \"AND\" in terms:\n",
    "        terms.remove(\"AND\")\n",
    "        result_set = set(inverted_index.get(terms[0], []))\n",
    "        for term in terms[1:]:\n",
    "            result_set &= set(inverted_index.get(term, []))\n",
    "    elif \"OR\" in terms:\n",
    "        result_set = set(inverted_index.get(terms[0], []))\n",
    "        for term in terms[1:]:\n",
    "            result_set |= set(inverted_index.get(term, []))\n",
    "    elif \"NOT\" in terms:\n",
    "        result_set = set(inverted_index.get(terms[0], []))\n",
    "        for term in terms[1:]:\n",
    "            result_set -= set(inverted_index.get(term, []))\n",
    "    else:\n",
    "        result_set = set(inverted_index.get(terms[0], []))\n",
    "        for term in terms[1:]:\n",
    "            result_set &= set(inverted_index.get(term, []))\n",
    "    \n",
    "    return list(result_set)\n",
    "\n",
    "# Παράδειγμα αναζήτησης με Boolean\n",
    "query_results = process_query(\"example AND query\")\n",
    "print(\"Αποτελέσματα αναζήτησης για το 'example AND query':\", query_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "963cba4d-a8fd-439f-84e9-64cfc127c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Κατάταξη αποτελεσμάτων για το 'example query': [9 8 7 6 5 4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Δημιουργία λίστας με περιεχόμενα των άρθρων\n",
    "documents = [' '.join(article['processed_tokens']) for article in articles]\n",
    "\n",
    "# Υπολογισμός TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "def rank_results(query):\n",
    "    query_vec = vectorizer.transform([' '.join(preprocess_text(query))])\n",
    "    scores = (tfidf_matrix * query_vec.T).toarray()\n",
    "    ranked_indices = scores.flatten().argsort()[::-1]\n",
    "    return ranked_indices\n",
    "\n",
    "# Παράδειγμα κατάταξης αποτελεσμάτων αναζήτησης\n",
    "ranked_results = rank_results(\"example query\")\n",
    "print(\"Κατάταξη αποτελεσμάτων για το 'example query':\", ranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04f30ee1-ebfc-4bfc-b04e-df41f78468ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Κατάταξη αποτελεσμάτων για το 'example': [9 8 7 6 5 4 3 2 1 0]\n",
      "Κατάταξη αποτελεσμάτων για το 'query': [9 8 7 6 5 4 3 2 1 0]\n",
      "Κατάταξη αποτελεσμάτων για το 'search': [9 8 7 6 5 4 3 2 1 0]\n",
      "Κατάταξη αποτελεσμάτων για το 'article': [1 3 6 9 8 7 4 5 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμαστική αναζήτηση με διαφορετικά ερωτήματα και κατάταξη\n",
    "queries = [\"example\", \"query\", \"search\", \"article\"]\n",
    "for query in queries:\n",
    "    ranked_results = rank_results(query)\n",
    "    print(f\"Κατάταξη αποτελεσμάτων για το '{query}':\", ranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bacb901c-3beb-44fc-a6d3-1b4102d28dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\inforetrieval\\mynewenv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\inforetrieval\\mynewenv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de7cd9c8-acbb-4402-8f4a-1de28c778b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd571a921cc4c7e9c951ede2df5b877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    if query:\n",
    "        with results_output:\n",
    "            ranked_results = rank_results(query)\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4d15c32-a439-489c-953c-f176b7a61509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Κατάταξη αποτελεσμάτων για το 'example': [9 8 7 6 5 4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμή της συνάρτησης rank_results\n",
    "test_query = \"example\"\n",
    "ranked_results = rank_results(test_query)\n",
    "\n",
    "print(f\"Κατάταξη αποτελεσμάτων για το '{test_query}':\", ranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abb5f928-3023-4526-8fdf-78fd9d7678f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text for 'example query': ['example', 'query']\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμή της συνάρτησης preprocess_text\n",
    "test_text = \"example query\"\n",
    "preprocessed_text = preprocess_text(test_text)\n",
    "\n",
    "print(f\"Preprocessed text for '{test_text}':\", preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3a8f1c9-d87c-4349-a176-2c8bdaeb3172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το 'example' βρέθηκε στο ανεστραμμένο ευρετήριο με ID άρθρων: [9, 9]\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμή του ανεστραμμένου ευρετηρίου\n",
    "test_word = \"example\"\n",
    "if test_word in inverted_index:\n",
    "    print(f\"Το '{test_word}' βρέθηκε στο ανεστραμμένο ευρετήριο με ID άρθρων:\", inverted_index[test_word])\n",
    "else:\n",
    "    print(f\"Το '{test_word}' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67304cf8-58c9-428a-8c68-cd6860279ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    if query:\n",
    "        with results_output:\n",
    "            ranked_results = rank_results(query)\n",
    "            if not ranked_results:  # Έλεγχος αν υπάρχουν αποτελέσματα\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b03253f2-6977-4307-a91b-886d7ef4ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d23ce8d79ab47cc815cef709e2734d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    if query:\n",
    "        with results_output:\n",
    "            ranked_results = rank_results(query)\n",
    "            if not ranked_results:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1771026-6a12-45d9-ab58-f78289275370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bc9c4395984cd3898145ee0bfe0add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    print(f\"Έγινε κλικ στο κουμπί αναζήτησης με ερώτημα: {query}\")  # Μήνυμα εκτύπωσης για έλεγχο\n",
    "    if query:\n",
    "        with results_output:\n",
    "            ranked_results = rank_results(query)\n",
    "            if not ranked_results:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35667e7a-98c0-4020-9136-f5a15b66ae6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63599260ab54cd09ab7be740672d0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Έγινε κλικ στο κουμπί αναζήτησης με ερώτημα: {query}\")  # Μήνυμα εκτύπωσης για έλεγχο\n",
    "        if query:\n",
    "            ranked_results = rank_results(query)\n",
    "            if not ranked_results:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf7ac345-2d3c-401f-93b9-9b37628d5f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text for 'science': ['science']\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμή της συνάρτησης preprocess_text\n",
    "test_text = \"science\"\n",
    "preprocessed_text = preprocess_text(test_text)\n",
    "\n",
    "print(f\"Preprocessed text for '{test_text}':\", preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a8c8021-4c64-4692-82db-12765831344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το 'science' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμή του ανεστραμμένου ευρετηρίου\n",
    "test_word = \"science\"\n",
    "if test_word in inverted_index:\n",
    "    print(f\"Το '{test_word}' βρέθηκε στο ανεστραμμένο ευρετήριο με ID άρθρων:\", inverted_index[test_word])\n",
    "else:\n",
    "    print(f\"Το '{test_word}' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f08227-6398-4020-9070-1eda05f9a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Κατάταξη αποτελεσμάτων για το 'science': [9 8 7 6 5 4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Δοκιμή της συνάρτησης rank_results\n",
    "test_query = \"science\"\n",
    "ranked_results = rank_results(test_query)\n",
    "\n",
    "print(f\"Κατάταξη αποτελεσμάτων για το '{test_query}':\", ranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3d292e2-1fe0-4358-9de2-bb8a3b7b04b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αριθμός άρθρων που περιέχουν τη λέξη 'science': 0\n"
     ]
    }
   ],
   "source": [
    "# Έλεγχος των άρθρων για τη λέξη \"science\"\n",
    "science_articles = [article for article in articles if 'science' in article['processed_tokens']]\n",
    "print(f\"Αριθμός άρθρων που περιέχουν τη λέξη 'science': {len(science_articles)}\")\n",
    "\n",
    "# Εμφάνιση τίτλων των άρθρων που περιέχουν τη λέξη \"science\"\n",
    "for i, article in enumerate(science_articles):\n",
    "    print(f\"Άρθρο {i+1} - Τίτλος: {article['title']}\")\n",
    "    print(f\"Περιεχόμενο: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "    print('\\n' + '-'*80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d24c8cd7-c116-4321-87f6-c244d41d57a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το αρχείο 'new_wikipedia_articles.json' δημιουργήθηκε και αποθηκεύτηκε επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Δημιουργία νέων άρθρων\n",
    "new_articles = [\n",
    "    {\n",
    "        \"title\": \"Science\",\n",
    "        \"content\": \"Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Technology\",\n",
    "        \"content\": \"Technology is the sum of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Artificial Intelligence\",\n",
    "        \"content\": \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Αποθήκευση των νέων άρθρων σε αρχείο JSON\n",
    "with open('new_wikipedia_articles.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(new_articles, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Το αρχείο 'new_wikipedia_articles.json' δημιουργήθηκε και αποθηκεύτηκε επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96f06d25-bb3d-4d46-803d-2f4cbc82d343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα άρθρα συνενώθηκαν επιτυχώς και αποθηκεύτηκαν στο αρχείο 'wikipedia_articles_updated.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Φόρτωση των υπάρχοντων άρθρων\n",
    "with open('wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Φόρτωση των νέων άρθρων\n",
    "with open('new_wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    new_articles = json.load(file)\n",
    "\n",
    "# Συνένωση των άρθρων\n",
    "articles.extend(new_articles)\n",
    "\n",
    "# Αποθήκευση των ενημερωμένων άρθρων σε αρχείο JSON\n",
    "with open('wikipedia_articles_updated.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Τα άρθρα συνενώθηκαν επιτυχώς και αποθηκεύτηκαν στο αρχείο 'wikipedia_articles_updated.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e9c909f-6d63-4aa7-b16f-140a582c1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Ορισμός λειτουργιών για προεπεξεργασία κειμένου\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Φόρτωση των ενημερωμένων άρθρων\n",
    "with open('wikipedia_articles_updated.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Προεπεξεργασία κειμένων\n",
    "for article in articles:\n",
    "    article['processed_tokens'] = preprocess_text(article['content'])\n",
    "\n",
    "print(\"Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb68464f-6fde-4bec-95bc-227f88652e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το ανεστραμμένο ευρετήριο δημιουργήθηκε επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "# Δημιουργία ανεστραμμένου ευρετηρίου\n",
    "inverted_index = {}\n",
    "\n",
    "for doc_id, article in enumerate(articles):\n",
    "    for word in article['processed_tokens']:\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(doc_id)\n",
    "\n",
    "print(\"Το ανεστραμμένο ευρετήριο δημιουργήθηκε επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "add78fd5-3359-4b10-b8b4-fa324eddfb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Κατάταξη αποτελεσμάτων για το 'science': [9 8 7 6 5 4 3 2 1 0]\n",
      "Title: Thomaston Historic District\n",
      "Content: The Thomaston Historic District encompasses much historic town center Thomaston Maine With settlement history dating 17th century town showcase 19thcentury architectural style 1870s The district extends 2 mile 32 km along United States Route 1 listed National Register Historic Places 19741 The town center Thomaston located one head Muscongus Bay...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Martin Jeitziner\n",
      "Content: Martin Jeitziner born 13 January 1963 Swiss former professional footballer played midfielder 1980s 1990s1 Jeitziner played youth football FC Basel advanced first team 198081 season headcoach Helmut Benthaus The team reigning Swiss champion Benthaus wanted integrate youngster team Jeitziner played domestic league debut club away game Charmilles Stadium 9 Mai...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Expected satiety\n",
      "Content: Expected satiety amount relief hunger expected particular food It closely associated expected satiation refers immediate fullness post meal food expected generate Scientists discovered food differ considerably expected satiety One estimate United Kingdom suggested may sixfold difference food commonly consumed compared calorie calorie1 This range variation important expected satiety thought good...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Jane Symons\n",
      "Content: Jane Symons born 1959 Australian medium consultant journalist author based London She written wide range newspaper magazine vice chair Medical Journalists Association publicpatient information lead CovidenceUK longitudinal study Her book How Have Baby Still Live Real World published country including UK USA Russia Symons edited health page The Sun1 2004...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Mamitha Baiju\n",
      "Content: Mamitha Baiju Indian actress working primarily Tamil film addition Malayalam film She made debut 2018 Sarvopari Palakkaran12 She well known character Operation Java 2021 Super Sharanya 2022 Pranaya Vilasam 2023 Her last release Premalu 2024 rank sixth highestgrossing Malayalam film time fourth highestgrossing Malayalam film 2024 Mamitha Baiju hail Kidangoor...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Συνάρτηση κατάταξης αποτελεσμάτων αναζήτησης\n",
    "def rank_results(query):\n",
    "    query_vec = vectorizer.transform([' '.join(preprocess_text(query))])\n",
    "    scores = (tfidf_matrix * query_vec.T).toarray()\n",
    "    ranked_indices = scores.flatten().argsort()[::-1]\n",
    "    return ranked_indices\n",
    "\n",
    "# Παράδειγμα αναζήτησης\n",
    "test_query = \"science\"\n",
    "ranked_results = rank_results(test_query)\n",
    "\n",
    "print(f\"Κατάταξη αποτελεσμάτων για το '{test_query}':\", ranked_results)\n",
    "for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "    article = articles[idx]\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "    print('\\n' + '-'*80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f4d4f-a458-4258-a7d4-e88490f6d076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1eceafd-f271-4fd9-b7df-3f23e3f19759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfad8961ecf64f2bbd70c77a87e4174f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Έγινε κλικ στο κουμπί αναζήτησης με ερώτημα: {query}\")  # Μήνυμα εκτύπωσης για έλεγχο\n",
    "        if query:\n",
    "            ranked_results = rank_results(query)\n",
    "            if not ranked_results:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c9ff7a7-0d40-49f1-a05e-eb265783f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το 'science' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\n",
      "Το 'technology' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\n",
      "Το 'artificial' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"science\", \"technology\", \"artificial\"]\n",
    "for word in test_words:\n",
    "    if word in inverted_index:\n",
    "        print(f\"Το '{word}' βρέθηκε στο ανεστραμμένο ευρετήριο με ID άρθρων:\", inverted_index[word])\n",
    "    else:\n",
    "        print(f\"Το '{word}' δεν βρέθηκε στο ανεστραμμένο ευρετήριο.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "309f017f-3668-42b5-bab4-ac2b55abc2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το αρχείο 'new_wikipedia_articles.json' δημιουργήθηκε και αποθηκεύτηκε επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Φόρτωση των νέων άρθρων\n",
    "new_articles = [\n",
    "    {\n",
    "        \"title\": \"Science\",\n",
    "        \"content\": \"Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Technology\",\n",
    "        \"content\": \"Technology is the sum of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Artificial Intelligence\",\n",
    "        \"content\": \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Αποθήκευση των νέων άρθρων σε αρχείο JSON\n",
    "with open('new_wikipedia_articles.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(new_articles, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Το αρχείο 'new_wikipedia_articles.json' δημιουργήθηκε και αποθηκεύτηκε επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33d2febb-d34d-4cdd-aa35-2cfecf009a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα άρθρα συνενώθηκαν επιτυχώς και αποθηκεύτηκαν στο αρχείο 'wikipedia_articles_updated.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Φόρτωση των υπάρχοντων άρθρων\n",
    "with open('wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Φόρτωση των νέων άρθρων\n",
    "with open('new_wikipedia_articles.json', 'r', encoding='utf-8') as file:\n",
    "    new_articles = json.load(file)\n",
    "\n",
    "# Συνένωση των άρθρων\n",
    "articles.extend(new_articles)\n",
    "\n",
    "# Αποθήκευση των ενημερωμένων άρθρων σε αρχείο JSON\n",
    "with open('wikipedia_articles_updated.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Τα άρθρα συνενώθηκαν επιτυχώς και αποθηκεύτηκαν στο αρχείο 'wikipedia_articles_updated.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0eb67af0-1c60-4472-b8d7-06328f1a8285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Ορισμός λειτουργιών για προεπεξεργασία κειμένου\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Φόρτωση των ενημερωμένων άρθρων\n",
    "with open('wikipedia_articles_updated.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Προεπεξεργασία κειμένων\n",
    "for article in articles:\n",
    "    article['processed_tokens'] = preprocess_text(article['content'])\n",
    "\n",
    "print(\"Τα άρθρα έχουν προεπεξεργαστεί επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26f83be1-4299-4719-96fd-dfe8e7146f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το ανεστραμμένο ευρετήριο δημιουργήθηκε επιτυχώς.\n"
     ]
    }
   ],
   "source": [
    "# Δημιουργία ανεστραμμένου ευρετηρίου\n",
    "inverted_index = {}\n",
    "\n",
    "for doc_id, article in enumerate(articles):\n",
    "    for word in article['processed_tokens']:\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(doc_id)\n",
    "\n",
    "print(\"Το ανεστραμμένο ευρετήριο δημιουργήθηκε επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9337a560-94da-4a62-8d97-e2c159115bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Κατάταξη αποτελεσμάτων για το 'science': [9 8 7 6 5 4 3 2 1 0]\n",
      "Κατάταξη αποτελεσμάτων για το 'technology': [9 8 7 6 5 4 3 2 1 0]\n",
      "Κατάταξη αποτελεσμάτων για το 'artificial intelligence': [9 8 7 6 5 4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\"science\", \"technology\", \"artificial intelligence\"]\n",
    "for query in test_queries:\n",
    "    ranked_results = rank_results(query)\n",
    "    print(f\"Κατάταξη αποτελεσμάτων για το '{query}':\", ranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a3402-aaff-4d76-8a62-2f820ba31231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f9850ce-51c5-4595-a89d-cea97d21f6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3569aeba7b4d5b8d3dc022efbb3004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Έγινε κλικ στο κουμπί αναζήτησης με ερώτημα: {query}\")  # Μήνυμα εκτύπωσης για έλεγχο\n",
    "        if query:\n",
    "            ranked_results = rank_results(query)\n",
    "            if not ranked_results:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf14c8db-8de8-4522-9c99-d093e6c18e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce308d89cb1949bfb9ee366b3f308053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "results_output = widgets.Output()\n",
    "\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Search button clicked with query: {query}\")\n",
    "        # Rank results and display\n",
    "        ranked_results = rank_results(query)\n",
    "        if not ranked_results:\n",
    "            print(\"No results found.\")\n",
    "        for idx in ranked_results[:5]:\n",
    "            article = articles[idx]\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")\n",
    "            print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24bd9c20-17af-477b-b853-a710bcf028df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed query: ['science']\n"
     ]
    }
   ],
   "source": [
    "test_query = \"science\"\n",
    "preprocessed_query = preprocess_text(test_query)\n",
    "print(f\"Preprocessed query: {preprocessed_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0cf538c5-851b-4443-9ef0-c930b99acd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286c207379684ef5bab3ad8ee9baaaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Create search box\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create search button\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Create results output area\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Function to handle search button click\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Search button clicked with query: {query}\")  # Debugging print statement\n",
    "        # Rank results and display\n",
    "        ranked_results = rank_results(query)\n",
    "        if not ranked_results:\n",
    "            print(\"No results found.\")\n",
    "        for idx in ranked_results[:5]:\n",
    "            article = articles[idx]\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")\n",
    "            print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Connect the button click event to the function\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Display the interface\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d21fa0b-9b90-476e-b9b2-f4b0d1647312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18deb0d612a43e2810f133545cbc22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Create search box\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create search button\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Create results output area\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Function to handle search button click\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Search button clicked with query: {query}\")  # Debugging print statement\n",
    "        if query:\n",
    "            ranked_results = rank_results(query)\n",
    "            print(f\"Ranked results: {ranked_results}\")  # Debugging print statement for results\n",
    "            if not ranked_results:\n",
    "                print(\"No results found.\")\n",
    "            for idx in ranked_results[:5]:  # Display top 5 results\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # First 50 words\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Connect the button click event to the function\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Display the interface\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a88c2b99-ef8a-4815-8186-727935836dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658f4ea5525f48fd9a0185db90cd50b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Input:', placeholder='Enter something'), Button(description='Click …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου εισαγωγής κειμένου\n",
    "input_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter something',\n",
    "    description='Input:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού\n",
    "button = widgets.Button(\n",
    "    description='Click me',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to see a message',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Περιοχή εμφάνισης αποτελεσμάτων\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Συνάρτηση που συνδέεται με το κουμπί\n",
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(\"Button clicked!\")\n",
    "\n",
    "# Σύνδεση του κουμπιού με τη συνάρτηση\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Εμφάνιση της διεπαφής\n",
    "display(VBox([input_box, button, output_area]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b1339d4-1732-41fd-ad79-05ac3bf82476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ddea403e304f77a2aa056f35ca5136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού υποβολής αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Search button clicked with query: {query}\")  # Μήνυμα εκτύπωσης για έλεγχο\n",
    "        if query:\n",
    "            # Προσομοίωση εμφάνισης αποτελεσμάτων\n",
    "            print(f\"Showing results for: {query}\")\n",
    "            for i in range(1, 4):\n",
    "                print(f\"Result {i} for query '{query}'\")\n",
    "            print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e36ea49-9021-45fc-b7f9-b7d19110331d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de46d6f663345a092dd2148424a044d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Input:', placeholder='Enter something'), Button(description='Click …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου εισαγωγής κειμένου\n",
    "input_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter something',\n",
    "    description='Input:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού\n",
    "button = widgets.Button(\n",
    "    description='Click me',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to see a message',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Περιοχή εμφάνισης αποτελεσμάτων\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Συνάρτηση που συνδέεται με το κουμπί\n",
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(\"Button clicked!\")\n",
    "\n",
    "# Σύνδεση του κουμπιού με τη συνάρτηση\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Εμφάνιση της διεπαφής\n",
    "display(VBox([input_box, button, output_area]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45a73a55-b5df-47d3-b0ae-84821707f498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c37630bbb144d588c6bacfd30c02ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', placeholder='Enter your query'), Button(description='Sear…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import VBox\n",
    "\n",
    "# Δημιουργία πεδίου αναζήτησης\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Δημιουργία κουμπιού αναζήτησης\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Δημιουργία πεδίου εμφάνισης αποτελεσμάτων\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# Συνάρτηση για την αναζήτηση και εμφάνιση των αποτελεσμάτων\n",
    "def on_search_button_clicked(b):\n",
    "    query = search_box.value\n",
    "    results_output.clear_output()\n",
    "    with results_output:\n",
    "        print(f\"Search button clicked with query: {query}\")  # Μήνυμα εκτύπωσης για έλεγχο\n",
    "        if query:\n",
    "            ranked_results = rank_results(query)\n",
    "            print(f\"Ranked results: {ranked_results}\")  # Μήνυμα εκτύπωσης για αποτελέσματα\n",
    "            if not ranked_results:\n",
    "                print(\"No results found.\")\n",
    "            for idx in ranked_results[:5]:  # Εμφάνιση των πρώτων 5 αποτελεσμάτων\n",
    "                article = articles[idx]\n",
    "                print(f\"Title: {article['title']}\")\n",
    "                print(f\"Content: {' '.join(article['processed_tokens'][:50])}...\")  # Πρώτες 50 λέξεις\n",
    "                print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "# Σύνδεση του κουμπιού υποβολής με τη συνάρτηση αναζήτησης\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Δημιουργία παραθύρου με το πεδίο αναζήτησης, το κουμπί και τα αποτελέσματα\n",
    "search_interface = VBox([search_box, search_button, results_output])\n",
    "\n",
    "# Εμφάνιση της διεπαφής αναζήτησης σε νέο παράθυρο\n",
    "display(search_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d7d3cd8-1dbf-4194-8072-c56acaf28056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902ab0042b0a427d9ca6f2aa30f30006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click me', icon='check', style=ButtonStyle(), tooltip='Click to see a message')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Δημιουργία κουμπιού\n",
    "button = widgets.Button(\n",
    "    description='Click me',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to see a message',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Συνάρτηση που συνδέεται με το κουμπί\n",
    "def on_button_clicked(b):\n",
    "    print(\"Button clicked!\")\n",
    "\n",
    "# Σύνδεση του κουμπιού με τη συνάρτηση\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Εμφάνιση του κουμπιού\n",
    "display(button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1d43ba-3a78-445b-a781-8e2f7d71910c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08437b639c074c2099c94f8516a810ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click me', icon='check', style=ButtonStyle(), tooltip='Click to see a message')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Δημιουργία κουμπιού\n",
    "button = widgets.Button(\n",
    "    description='Click me',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to see a message',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Συνάρτηση που συνδέεται με το κουμπί\n",
    "def on_button_clicked(b):\n",
    "    print(\"Button clicked!\")\n",
    "\n",
    "# Σύνδεση του κουμπιού με τη συνάρτηση\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Εμφάνιση του κουμπιού\n",
    "display(button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6056853-36c1-43e8-8004-26c4c676972d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
